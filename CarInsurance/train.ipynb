{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('VI_train.csv', index_col='id').iloc[:,1:]\n",
    "df_test = pd.read_csv('VI_test.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:, -1]\n",
    "X_test = df_test\n",
    "\n",
    "GENDER_MAPPING = {'Male': 0, 'Female': 1}\n",
    "VEHICLE_AGE_MAPPING = {'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2}\n",
    "VEHICLE_DAMAGE = {'Yes': 0, 'No': 1}\n",
    "X = X.replace({'Gender': GENDER_MAPPING, 'Vehicle_Age': VEHICLE_AGE_MAPPING, 'Vehicle_Damage':VEHICLE_DAMAGE})\n",
    "X_test = X_test.replace({'Gender': GENDER_MAPPING, 'Vehicle_Age': VEHICLE_AGE_MAPPING, 'Vehicle_Damage':VEHICLE_DAMAGE})\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X,y,\n",
    "                                                      test_size=0.25,\n",
    "                                                      random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf, X_valid, y_valid):\n",
    "    y_pred = clf.predict(X_valid)\n",
    "    y_true = y_valid\n",
    "    precision = sum([i and j for i, j in zip(y_true, y_pred)]) / sum(y_pred)\n",
    "    recall = sum([i and j for i, j in zip(y_true, y_pred)]) / sum(y_true)\n",
    "    f1_score = 2 * (recall * precision) / (recall + precision)\n",
    "    print('f1_score =', f1_score)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def submit(cls, X_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    with open('submission.json', 'w') as f:\n",
    "        json.dump({str(k):int(v) for k,v in zip(X_test.index, y_pred)}, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train_balance = pd.concat([X_train[y_train == 0][:27727], X_train[y_train == 1]])\n",
    "y_train_balance = pd.Series(np.concatenate([np.zeros(X_train[y_train == 0][:27727].shape[0]), \n",
    "                                            np.ones(X_train[y_train == 1].shape[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决策树： F1 score = 0.2937212536100117\n",
    "# X_train 欠采样：F1 score = 0.38390311296628354\n",
    "if False:\n",
    "    from sklearn import tree\n",
    "    clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "    clf = clf.fit(X_train_balance, y_train_balance)\n",
    "    # clf.score(X_valid, y_valid)\n",
    "    evaluate(clf, X_valid, y_valid)\n",
    "    submit(clf, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score = 0.4101358835129805\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost 欠采样 0.40685683674835943\n",
    "if True:\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn import tree\n",
    "    clf = AdaBoostClassifier(tree.DecisionTreeClassifier(criterion='entropy'))\n",
    "    clf = clf.fit(X_train_balance, y_train_balance)\n",
    "    evaluate(clf, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EasyEnsembleClassifier + DecisionTreeClassifier: 0.4287806664225558\n",
    "if False:\n",
    "    from imblearn.ensemble import EasyEnsembleClassifier     \n",
    "    from sklearn import tree\n",
    "    clf = EasyEnsembleClassifier(n_estimators=20,\n",
    "                                 base_estimator=tree.DecisionTreeClassifier(criterion='entropy'),\n",
    "                                 random_state=42)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    evaluate(clf, X_valid, y_valid)\n",
    "    submit(clf, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EasyEnsembleClassifier + DecisionTreeClassifier: 0.4287806664225558\n",
    "if False:\n",
    "    from imblearn.ensemble import EasyEnsembleClassifier     \n",
    "    from sklearn import tree\n",
    "    clf = EasyEnsembleClassifier(n_estimators=30,\n",
    "                                 base_estimator=AdaBoostClassifier(tree.DecisionTreeClassifier(criterion='entropy')),\n",
    "                                 random_state=42)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    evaluate(clf, X_valid, y_valid)\n",
    "    submit(clf, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/colorfulgreen/.local/lib/python3.8/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:40:17] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "f1_score = 0.43695201657898936\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier(scale_pos_weight=7.1)\n",
    "model.fit(X_train, y_train)\n",
    "evaluate(model, X_valid, y_valid)\n",
    "submit(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/colorfulgreen/.local/lib/python3.8/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:36:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "f1_score = 0.43695201657898936\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier(scale_pos_weight=7.1)\n",
    "model.fit(X_train, y_train)\n",
    "evaluate(model, X_valid, y_valid)\n",
    "submit(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
